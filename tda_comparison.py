# -*- coding: utf-8 -*-
"""TDA-comparison-Clustering-vs-Persistent-Homology-Mapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1APrU5_RicTD6gW-oucas2RympkPnPps6

# **TDA Comparison: Clustering vs. Persistent Homology vs.Mapper**

This script demonstrates and compares three approaches to analyzing the structure
of a "noisy circle" dataset:

**1**.**K-Means Clustering**: A traditional clustering algorithm.

**2.** **Persistent Homology (via giotto-tda)**: A core TDA technique to find
   topological features like connected components and loops.

**3.** **Mapper (via kmapper):** A TDA tool that constructs a graph representation (simplicial
   complex) of the data, often revealing its shape.

#The goal is to show how TDA methods can capture global topological features
# (like the circle's loop) that might be missed or broken by standard clustering.
"""

#!pip install numpy matplotlib scikit-learn giotto-tda kmapper

# Standard library imports
import webbrowser
from typing import Tuple, List, Dict, Any

# Third-party library imports
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# TDA specific imports
import kmapper as km
from gtda.homology import VietorisRipsPersistence

# --- Configuration Constants ---
# Data Generation
N_POINTS_CIRCLE: int = 300
CIRCLE_RADIUS: float = 5.0
NOISE_LEVEL: float = 0.3  # Noise standard deviation for the circle

# K-Means Clustering
N_CLUSTERS_KMEANS: int = 4

# Persistent Homology
HOMOLOGY_DIMENSIONS: Tuple[int, int] = (0, 1)  # Compute H0 (components) and H1 (loops)

# KeplerMapper
KM_LENS_N_COMPONENTS: int = 1  # Number of PCA components for the lens
KM_COVER_N_CUBES: int = 10     # Number of intervals in the cover
KM_COVER_PERC_OVERLAP: float = 0.5 # Percentage overlap in the cover
KM_DBSCAN_EPS: float = 0.4     # Epsilon for DBSCAN clustering in Mapper
KM_DBSCAN_MIN_SAMPLES: int = 5 # Min_samples for DBSCAN in Mapper
KM_OUTPUT_FILENAME: str = "kepler_mapper_noisy_circle.html"

# Plotting
FIG_SIZE_MATPLOTLIB: Tuple[float, float] = (14, 6.5) # Figure size for KMeans vs PD plot
RANDOM_STATE: int = 42 # For reproducible results in algorithms like KMeans, PCA


def print_library_versions():
    """Prints versions of key libraries being used."""
    import gtda
    import scipy
    import sklearn
    print("--- Library Versions ---")
    print(f"giotto-tda version: {gtda.__version__}")
    print(f"numpy version: {np.__version__}")
    print(f"scipy version: {scipy.__version__}")
    print(f"scikit-learn version: {sklearn.__version__}")
    print(f"matplotlib version: {matplotlib.__version__}")
    print(f"kepler-mapper version: {km.__version__}")
    print("-" * 26 + "\n")

def generate_noisy_circle(n_points: int, radius: float, noise_std: float) -> np.ndarray:
    """
    Generates points forming a circle with added Gaussian noise.

    Args:
        n_points: Number of data points.
        radius: Radius of the perfect circle.
        noise_std: Standard deviation of the Gaussian noise.

    Returns:
        A NumPy array of shape (n_points, 2) containing the (x, y) coordinates.
    """
    angles = np.random.uniform(0, 2 * np.pi, n_points)
    x_perfect = radius * np.cos(angles)
    y_perfect = radius * np.sin(angles)
    x_noisy = x_perfect + np.random.normal(0, noise_std, n_points)
    y_noisy = y_perfect + np.random.normal(0, noise_std, n_points)
    return np.stack((x_noisy, y_noisy), axis=1)


def run_kmeans_clustering(data: np.ndarray, n_clusters: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    Performs K-Means clustering on the given data.

    Args:
        data: Input data (n_samples, n_features).
        n_clusters: The number of clusters to form.

    Returns:
        A tuple containing:
            - cluster_labels (np.ndarray): Cluster labels for each point.
            - cluster_centers (np.ndarray): Coordinates of cluster centers.
    """
    kmeans_model = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init='auto')
    cluster_labels = kmeans_model.fit_predict(data)
    cluster_centers = kmeans_model.cluster_centers_
    print(f"Applied K-Means with k={n_clusters}.\n")
    return cluster_labels, cluster_centers


def compute_persistence_diagram(data: np.ndarray, homology_dims: Tuple[int, ...]) -> np.ndarray:
    """
    Computes the persistence diagram for the given data using Vietoris-Rips.

    Args:
        data: Input data (n_samples, n_features).
        homology_dims: A tuple of homology dimensions to compute (e.g., (0, 1)).

    Returns:
        A NumPy array representing the persistence diagram, or an empty array on error.
        Each row is [birth_scale, death_scale, homology_dimension].
    """
    vr_computer = VietorisRipsPersistence(
        homology_dimensions=homology_dims,
        coeff=2, n_jobs=-1, collapse_edges=True, max_edge_length=np.inf
    )
    print("--- TDA: Computing Persistence Diagram ---")
    try:
        # Input to fit_transform must be a list of point clouds
        persistence_diagram_list = vr_computer.fit_transform([data])
        diagram_data = persistence_diagram_list[0]
        print(f"Persistence diagram computed with {diagram_data.shape[0]} features.\n")
        return diagram_data
    except Exception as e:
        print(f"ERROR during VietorisRipsPersistence computation: {e}")
        return np.array([])


def analyze_persistence_diagram(diagram_data: np.ndarray):
    """Prints insights from the persistence diagram."""
    if not (isinstance(diagram_data, np.ndarray) and diagram_data.size > 0 and diagram_data.shape[1] == 3):
        print("WARNING: Persistence diagram data is empty or invalid for analysis.\n")
        return

    h0_features = diagram_data[diagram_data[:, 2] == 0]
    h1_features = diagram_data[diagram_data[:, 2] == 1]

    print("--- TDA: Persistence Diagram Insights ---")
    if len(h0_features) > 0:
        inf_death_h0 = h0_features[np.isinf(h0_features[:, 1])]
        if len(inf_death_h0) > 0:
            print(f"H0: Found {len(inf_death_h0)} component(s) with infinite persistence: {inf_death_h0}")
        else:
            finite_death_h0 = h0_features[np.isfinite(h0_features[:, 1])]
            if len(finite_death_h0) > 0:
                most_persistent_finite_h0 = finite_death_h0[np.argmax(finite_death_h0[:, 1] - finite_death_h0[:, 0])]
                print(f"H0: Most persistent FINITE component: Birth={most_persistent_finite_h0[0]:.2f}, Death={most_persistent_finite_h0[1]:.2f}")
    else:
        print("H0: No 0-dimensional features found.")

    if len(h1_features) > 0:
        h1_persistence_values = h1_features[:, 1] - h1_features[:, 0]
        valid_h1_mask = np.isfinite(h1_persistence_values) & (h1_persistence_values > 1e-9)
        if np.any(valid_h1_mask):
            most_persistent_h1_idx = np.argmax(h1_persistence_values[valid_h1_mask])
            most_persistent_h1 = h1_features[valid_h1_mask][most_persistent_h1_idx]
            print(f"H1: Most persistent loop: Birth={most_persistent_h1[0]:.2f}, Death={most_persistent_h1[1]:.2f}, "
                  f"Persistence={(most_persistent_h1[1] - most_persistent_h1[0]):.2f}")
        else:
            print("H1: Loops found, but none with significant finite positive persistence.")
    else:
        print("H1: No 1-dimensional features (loops) found.")
    print("-" * 40 + "\n")


def run_kepler_mapper(data: np.ndarray, lens_n_components: int,
                      cover_n_cubes: int, cover_perc_overlap: float,
                      dbscan_eps: float, dbscan_min_samples: int,
                      output_filename: str) -> Dict[str, Any]:
    """
    Constructs and visualizes a KeplerMapper graph.

    Args:
        data: Input data (n_samples, n_features).
        lens_n_components: Number of PCA components for the lens.
        cover_n_cubes: Number of intervals in the cover.
        cover_perc_overlap: Percentage overlap in the cover.
        dbscan_eps: Epsilon for DBSCAN clustering.
        dbscan_min_samples: Min_samples for DBSCAN clustering.
        output_filename: Filename for the HTML output.

    Returns:
        The KeplerMapper graph dictionary.
    """
    print("--- TDA: KeplerMapper ---")
    mapper = km.KeplerMapper(verbose=1)

    # Define lens (projection)
    pca = PCA(n_components=lens_n_components, random_state=RANDOM_STATE)
    lens_array = pca.fit_transform(data) # For map function
    lens_1d_for_coloring = lens_array[:, 0].ravel() if lens_n_components > 0 else np.zeros(data.shape[0]) # For visualize color_values

    # Define cover
    cover = km.Cover(n_cubes=cover_n_cubes, perc_overlap=cover_perc_overlap)

    # Define clustering algorithm
    cluster_algorithm = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples)

    # Create graph
    graph = mapper.map(
        lens=lens_array, X=data, cover=cover, clusterer=cluster_algorithm
    )

    print(f"Visualizing KeplerMapper graph. Output will be saved to: {output_filename}")
    # KeplerMapper's visualize expects 1D array for color_values, representing values for original points
    mapper.visualize(
        graph,
        path_html=output_filename,
        title=f"KeplerMapper: Noisy Circle (Lens: PCA {lens_n_components}D)",
        # Default tooltips are fine (node_id, size)
        color_values=lens_1d_for_coloring,
        color_function_name=f'Avg. Lens (PCA Comp 1 of {lens_n_components})'
    )
    print(f"KeplerMapper graph saved. Attempting to open in browser...\n")
    try:
        webbrowser.open_new_tab(output_filename)
    except Exception as e:
        print(f"Could not open browser automatically: {e}. Please open '{output_filename}' manually.")
    return graph


def plot_results(data_scaled: np.ndarray,
                 kmeans_labels: np.ndarray, kmeans_centers: np.ndarray,
                 pd_data: np.ndarray):
    """
    Generates Matplotlib plots for K-Means and Persistence Diagram.
    """
    print("--- Matplotlib Visualizations ---")
    fig, axes = plt.subplots(1, 2, figsize=FIG_SIZE_MATPLOTLIB)
    fig.suptitle("KMeans Clustering vs. Persistence Diagram on Noisy Circle", fontsize=16, y=1.03)

    # Plot 1: K-Means Clustering
    ax_kmeans = axes[0]
    scatter_kmeans = ax_kmeans.scatter(data_scaled[:, 0], data_scaled[:, 1], c=kmeans_labels,
                                       cmap='viridis', s=20, alpha=0.7)
    ax_kmeans.scatter(kmeans_centers[:, 0], kmeans_centers[:, 1], c='red', s=150, marker='X',
                      edgecolor='black', label='Centroids')
    ax_kmeans.set_title(f'K-Means Clustering (k={N_CLUSTERS_KMEANS})')
    ax_kmeans.set_xlabel('X (scaled)'); ax_kmeans.set_ylabel('Y (scaled)')
    handles_k, _ = scatter_kmeans.legend_elements(prop="colors", alpha=0.7)
    labels_k = [f"Cluster {i}" for i in np.unique(kmeans_labels)]
    centroid_artist_k = next((c for c in reversed(ax_kmeans.collections) if hasattr(c, 'get_label') and c.get_label() == 'Centroids'), None)
    if centroid_artist_k:
        handles_k.append(centroid_artist_k); labels_k.append(centroid_artist_k.get_label())
    ax_kmeans.legend(handles_k, labels_k, title="Legend", loc="best")
    ax_kmeans.set_aspect('equal', adjustable='box')
    ax_kmeans.grid(True, linestyle='--', alpha=0.5)

    # Plot 2: Persistence Diagram (Manual Plot)
    ax_pd = axes[1]
    if isinstance(pd_data, np.ndarray) and pd_data.size > 0 and pd_data.shape[1] == 3:
        ax_pd.set_title('Persistence Diagram (Manual Plot)')
        ax_pd.set_xlabel('Birth Scale'); ax_pd.set_ylabel('Death Scale')
        h0 = pd_data[pd_data[:, 2] == 0]
        h1 = pd_data[pd_data[:, 2] == 1]
        if len(h0) > 0: ax_pd.scatter(h0[:, 0], h0[:, 1], c='blue', marker='o', label='H0', alpha=0.6, s=25, ec='k', lw=0.5)
        if len(h1) > 0: ax_pd.scatter(h1[:, 0], h1[:, 1], c='red', marker='^', label='H1', alpha=0.7, s=40, ec='k', lw=0.5)

        finite_pts = pd_data[np.isfinite(pd_data[:, :2]).all(axis=1)]
        if finite_pts.size > 0:
            min_v, max_v = np.min(finite_pts[:,:2]), np.max(finite_pts[:,:2])
            plot_min, plot_max = min(0.0, min_v), max_v
            buf = (plot_max - plot_min) * 0.1 + 0.1
            lim_min, lim_max = plot_min - buf, plot_max + buf
            if lim_max <= lim_min: lim_max = lim_min + 1.0
            ax_pd.plot([lim_min, lim_max], [lim_min, lim_max], 'k--', alpha=0.5, label='y=x')
            ax_pd.set_xlim(lim_min, lim_max); ax_pd.set_ylim(lim_min, lim_max)
        else:
            ax_pd.plot([0,1],[0,1],'k--',alpha=0.5,label='y=x'); ax_pd.set_xlim(0,1); ax_pd.set_ylim(0,1)
        ax_pd.legend(loc='lower right', fontsize='small')
        ax_pd.grid(True, linestyle=':', alpha=0.6)
    else:
        ax_pd.text(0.5,0.5,"PD data invalid.",ha='center',va='center',transform=ax_pd.transAxes,c='red')
        ax_pd.set_title('Persistence Diagram - ERROR')

    plt.tight_layout(pad=2.0)
    plt.show()
    print("Matplotlib plots displayed.\n")


def print_interpretation_summary():
    """Prints a summary of the expected results and their interpretation."""
    print("\n--- Interpretation Summary ---")
    print("1. Noisy Circle Data: Points forming a circular shape with noise.")
    print("\n2. K-Means Clustering:")
    print(f"   - Partitions data into {N_CLUSTERS_KMEANS} segments, potentially breaking the circle's global structure.")
    print("\n3. TDA - Persistence Diagram:")
    print("   - H0 (Blue Dots): Connected components. One should persist long, representing the whole dataset.")
    print("   - H1 (Red Triangles): Loops. One H1 point far from the diagonal should robustly identify the circle's central hole.")
    print("\n4. TDA - KeplerMapper Graph (see HTML file):")
    print(f"   - Generates a graph ('{KM_OUTPUT_FILENAME}') summarizing the data's shape.")
    print("   - The graph should ideally forms a loop, visually representing the 1D homology (circle's hole).")
    print("   - Node colors (based on PCA lens) show how the data is 'sliced' and mapped.")
    print("   - Tuning KeplerMapper parameters (lens, cover, clustering) may be required.")
    print("\nConclusion:")
    print("   - Clustering groups by proximity, may miss global shapes.")
    print("   - Persistent Homology quantifies topological features and their significance.")
    print("   - KeplerMapper offers a graph-based, often intuitive, summary of data topology.")


def main():
    """Main function to execute the TDA comparison workflow."""
    print_library_versions()

    # 1. Generate and Prepare Data
    print("--- 1. Data Generation & Preparation ---")
    raw_data = generate_noisy_circle(N_POINTS_CIRCLE, CIRCLE_RADIUS, NOISE_LEVEL)
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(raw_data)
    print(f"Generated {scaled_data.shape[0]} points. Data scaled.\n")

    # 2. K-Means Clustering
    print("--- 2. K-Means Clustering ---")
    kmeans_labels, kmeans_centers = run_kmeans_clustering(scaled_data, N_CLUSTERS_KMEANS)

    # 3. Persistent Homology
    pd_data = compute_persistence_diagram(scaled_data, HOMOLOGY_DIMENSIONS)
    analyze_persistence_diagram(pd_data)

    # 4. KeplerMapper
    _ = run_kepler_mapper(
        data=scaled_data,
        lens_n_components=KM_LENS_N_COMPONENTS,
        cover_n_cubes=KM_COVER_N_CUBES,
        cover_perc_overlap=KM_COVER_PERC_OVERLAP,
        dbscan_eps=KM_DBSCAN_EPS,
        dbscan_min_samples=KM_DBSCAN_MIN_SAMPLES,
        output_filename=KM_OUTPUT_FILENAME
    ) # Graph object is returned but not explicitly used further here

    # 5. Matplotlib Visualizations
    plot_results(scaled_data, kmeans_labels, kmeans_centers, pd_data)

    # 6. Interpretation
    print_interpretation_summary()

if __name__ == "__main__":
    main()

